{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9063603-7936-49c9-ab63-e672632b7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_polynomial_regression(arrays, batch_info, degree=3, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Perform Bayesian polynomial regression on multiple arrays and assign probabilities\n",
    "    to each array being a true signal.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arrays : list of np.ndarray\n",
    "        List of 1x100 arrays containing the data\n",
    "    batch_info : list\n",
    "        List of batch labels corresponding to each array\n",
    "    degree : int\n",
    "        Degree of polynomial to fit (default: 3)\n",
    "    n_samples : int\n",
    "        Number of MCMC samples (default: 1000)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict:\n",
    "        coefficients: np.ndarray - Mean polynomial coefficients\n",
    "        array_probs: np.ndarray - Probability of each array being true signal\n",
    "        trace: arviz.InferenceData - MCMC trace for further analysis\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pymc as pm\n",
    "    import arviz as az\n",
    "    from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "    \n",
    "    # Convert inputs to numpy arrays\n",
    "    X = np.arange(100).reshape(-1, 1)\n",
    "    arrays = np.array(arrays)\n",
    "    batch_info = np.array(batch_info)\n",
    "    n_arrays = len(arrays)\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_poly)\n",
    "    \n",
    "    # Build Bayesian model\n",
    "    with pm.Model() as model:\n",
    "        # Global parameters\n",
    "        beta = pm.Normal('beta', mu=0, sigma=1, shape=degree+1)\n",
    "        sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "        \n",
    "        # Batch effects\n",
    "        batch_effect = pm.Normal('batch_effect', mu=0, sigma=0.5, \n",
    "                               shape=len(np.unique(batch_info)))\n",
    "        \n",
    "        # Signal probability for each array\n",
    "        array_weight = pm.Beta('array_weight', alpha=2, beta=2, shape=n_arrays)\n",
    "        \n",
    "        # Expected value\n",
    "        mu = pm.Deterministic('mu', pm.math.dot(X_scaled, beta))\n",
    "        \n",
    "        # Add batch effects to expected value\n",
    "        mu_with_batch = mu + batch_effect[batch_info][:, None]\n",
    "        \n",
    "        # Likelihood\n",
    "        for i in range(n_arrays):\n",
    "            signal = pm.Normal(f'signal_{i}', \n",
    "                             mu=mu_with_batch[i], \n",
    "                             sigma=sigma, \n",
    "                             observed=arrays[i],\n",
    "                             shape=100)\n",
    "        \n",
    "        # Sample from the model\n",
    "        trace = pm.sample(n_samples, return_inferencedata=True)\n",
    "    \n",
    "    # Extract results correctly\n",
    "    array_probs = trace.posterior['array_weight'].mean(dim=['chain', 'draw']).values\n",
    "    coef_means = trace.posterior['beta'].mean(dim=['chain', 'draw']).values\n",
    "    \n",
    "    # Transform coefficients back to original scale\n",
    "    final_coefs = coef_means / scaler.scale_[1:]\n",
    "    \n",
    "    return {\n",
    "        'coefficients': final_coefs,\n",
    "        'array_probs': array_probs,\n",
    "        'trace': trace\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bb0d0a-4631-4c94-9e97-9d04c76b33f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [beta, sigma, batch_effect, array_weight]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0608adae914e69a2902c1207b29ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 45 seconds.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     arrays\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m     23\u001b[0m     batch_info\u001b[38;5;241m.\u001b[39mappend(i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mbayesian_polynomial_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 78\u001b[0m, in \u001b[0;36mbayesian_polynomial_regression\u001b[0;34m(arrays, batch_info, degree, n_samples)\u001b[0m\n\u001b[1;32m     75\u001b[0m coef_means \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mposterior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdraw\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Transform coefficients back to original scale\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m final_coefs \u001b[38;5;241m=\u001b[39m \u001b[43mcoef_means\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoefficients\u001b[39m\u001b[38;5;124m'\u001b[39m: final_coefs,\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray_probs\u001b[39m\u001b[38;5;124m'\u001b[39m: array_probs,\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m'\u001b[39m: trace\n\u001b[1;32m     84\u001b[0m }\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,) (3,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "n_arrays = 20\n",
    "x = np.linspace(0, 1, 100)\n",
    "true_signal = 3*x**2 - 2*x + 1\n",
    "\n",
    "arrays = []\n",
    "batch_info = []\n",
    "\n",
    "# Generate some clean signals with batch effects\n",
    "for i in range(n_arrays):\n",
    "    batch = i % 3\n",
    "    batch_effect = np.random.normal(batch*0.5, 0.1, 100)\n",
    "    noise = np.random.normal(0, 0.1, 100)\n",
    "    signal = true_signal + batch_effect + noise\n",
    "    arrays.append(signal)\n",
    "    batch_info.append(batch)\n",
    "\n",
    "# Add some pure noise arrays\n",
    "for i in range(5):\n",
    "    arrays.append(np.random.normal(0, 1, 100))\n",
    "    batch_info.append(i % 3)\n",
    "\n",
    "results = bayesian_polynomial_regression(arrays, batch_info)\n",
    "\n",
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(results['array_probs'], 'o-')\n",
    "plt.title('Probability of Each Array Being Signal')\n",
    "plt.xlabel('Array Index')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.subplot(122)\n",
    "x_plot = np.linspace(0, 100, 100)\n",
    "y_pred = np.polyval(results['coefficients'][::-1], x_plot)\n",
    "plt.plot(x_plot, y_pred, 'r-', label='Fitted Polynomial')\n",
    "plt.title('Fitted Polynomial')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73860725-30db-4575-91ba-a91e75fa04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_polynomial_regression(arrays, batch_info, degree=3, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Perform Bayesian polynomial regression on multiple arrays and assign probabilities\n",
    "    to each array being a true signal.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arrays : list of np.ndarray\n",
    "        List of 1x100 arrays containing the data\n",
    "    batch_info : list\n",
    "        List of batch labels corresponding to each array\n",
    "    degree : int\n",
    "        Degree of polynomial to fit (default: 3)\n",
    "    n_samples : int\n",
    "        Number of MCMC samples (default: 1000)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pymc as pm\n",
    "    import arviz as az\n",
    "    from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "    \n",
    "    # Convert inputs to numpy arrays\n",
    "    X = np.arange(100).reshape(-1, 1)\n",
    "    arrays = np.array(arrays)\n",
    "    batch_info = np.array(batch_info)\n",
    "    n_arrays = len(arrays)\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_poly)\n",
    "    \n",
    "    # Build Bayesian model\n",
    "    with pm.Model() as model:\n",
    "        # Global parameters\n",
    "        beta = pm.Normal('beta', mu=0, sigma=1, shape=degree+1)\n",
    "        sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "        \n",
    "        # Batch effects\n",
    "        batch_effect = pm.Normal('batch_effect', mu=0, sigma=0.5, \n",
    "                               shape=len(np.unique(batch_info)))\n",
    "        \n",
    "        # Signal probability for each array\n",
    "        array_weight = pm.Beta('array_weight', alpha=2, beta=2, shape=n_arrays)\n",
    "        \n",
    "        # Expected value\n",
    "        mu = pm.Deterministic('mu', pm.math.dot(X_scaled, beta))\n",
    "        \n",
    "        # Add batch effects to expected value\n",
    "        mu_with_batch = mu + batch_effect[batch_info][:, None]\n",
    "        \n",
    "        # Likelihood\n",
    "        for i in range(n_arrays):\n",
    "            signal = pm.Normal(f'signal_{i}', \n",
    "                             mu=mu_with_batch[i], \n",
    "                             sigma=sigma, \n",
    "                             observed=arrays[i],\n",
    "                             shape=100)\n",
    "        \n",
    "        # Sample from the model\n",
    "        trace = pm.sample(n_samples, return_inferencedata=True)\n",
    "    \n",
    "    # Extract results correctly\n",
    "    array_probs = trace.posterior['array_weight'].mean(dim=['chain', 'draw']).values\n",
    "    coef_means = trace.posterior['beta'].mean(dim=['chain', 'draw']).values\n",
    "    \n",
    "    # Transform coefficients back to original scale\n",
    "    # No need to scale coefficients as we'll use the standardized features for prediction\n",
    "    \n",
    "    return {\n",
    "        'coefficients': coef_means,\n",
    "        'array_probs': array_probs,\n",
    "        'trace': trace,\n",
    "        'scaler': scaler,\n",
    "        'poly': poly\n",
    "    }\n",
    "\n",
    "def predict_polynomial(results, x):\n",
    "    \"\"\"\n",
    "    Make predictions using the fitted polynomial model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Results from bayesian_polynomial_regression\n",
    "    x : array-like\n",
    "        X values to predict\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    y_pred : np.ndarray\n",
    "        Predicted y values\n",
    "    \"\"\"\n",
    "    # Transform x using the same polynomial features and scaling\n",
    "    X_poly = results['poly'].transform(x.reshape(-1, 1))\n",
    "    X_scaled = results['scaler'].transform(X_poly)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = np.dot(X_scaled, results['coefficients'])\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c46e38-6632-4179-937a-d1c2300ce6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [beta, sigma, batch_effect, array_weight]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925aca3f88e84dd99c8956c9ceb629f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "n_arrays = 20\n",
    "x = np.linspace(0, 1, 100)\n",
    "true_signal = 3*x**2 - 2*x + 1\n",
    "\n",
    "arrays = []\n",
    "batch_info = []\n",
    "\n",
    "# Generate some clean signals with batch effects\n",
    "for i in range(n_arrays):\n",
    "    batch = i % 3\n",
    "    batch_effect = np.random.normal(batch*0.5, 0.1, 100)\n",
    "    noise = np.random.normal(0, 0.1, 100)\n",
    "    signal = true_signal + batch_effect + noise\n",
    "    arrays.append(signal)\n",
    "    batch_info.append(batch)\n",
    "\n",
    "# Add some pure noise arrays\n",
    "for i in range(5):\n",
    "    arrays.append(np.random.normal(0, 1, 100))\n",
    "    batch_info.append(i % 3)\n",
    "\n",
    "results = bayesian_polynomial_regression(arrays, batch_info)\n",
    "\n",
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot array probabilities\n",
    "plt.subplot(121)\n",
    "plt.plot(results['array_probs'], 'o-')\n",
    "plt.title('Probability of Each Array Being Signal')\n",
    "plt.xlabel('Array Index')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "# Plot fitted polynomial\n",
    "plt.subplot(122)\n",
    "x_plot = np.linspace(0, 100, 100)\n",
    "y_pred = predict_polynomial(results, x_plot)\n",
    "plt.plot(x_plot, y_pred, 'r-', label='Fitted Polynomial')\n",
    "plt.title('Fitted Polynomial')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dee3ba-d692-4d84-a35a-0b181c8d0b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
