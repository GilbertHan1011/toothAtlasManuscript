{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfede697-d1c3-4b8f-9059-0ed73e7507ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, m, latent_dim=10):\n",
    "        \"\"\"\n",
    "        Variational Autoencoder for learning the distribution of N arrays.\n",
    "\n",
    "        Args:\n",
    "            m (int): Number of columns in matrix M (same as the length of array N).\n",
    "            latent_dim (int, optional): Dimension of the latent space. Defaults to 10.\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        self.m = m\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(m, 128)\n",
    "        self.fc2_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(128, latent_dim)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim, 128)\n",
    "        self.fc4 = nn.Linear(128, m)\n",
    "\n",
    "        # Polynomial Regression Parameter G\n",
    "        self.G = nn.Parameter(torch.randn(3, m))\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        mu = self.fc2_mu(h1)\n",
    "        logvar = self.fc2_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from N(0,1).\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return self.fc4(h3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        N = self.decode(z)\n",
    "        # Compute predictions using G\n",
    "        N_poly = torch.stack([\n",
    "            torch.ones_like(N),\n",
    "            N,\n",
    "            N ** 2\n",
    "        ], dim=0)  # Shape: (3, batch_size, m)\n",
    "        G_expanded = self.G.unsqueeze(1)  # Shape: (3, 1, m)\n",
    "        predictions = (G_expanded * N_poly).sum(dim=0)  # Shape: (batch_size, m)\n",
    "        return predictions, mu, logvar, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d6e2a2-4666-4339-a7b8-b969cc4b203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PolynomialVaeDataset(Dataset):\n",
    "    def __init__(self, M_list):\n",
    "        \"\"\"\n",
    "        Custom Dataset for VAE-based polynomial regression.\n",
    "\n",
    "        Args:\n",
    "            M_list (list of torch.Tensor): List of M matrices, each of shape (n, m).\n",
    "        \"\"\"\n",
    "        self.M_list = M_list  # List of tensors with shape (n, m)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.M_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For VAEs, we can use the mean of M as the target\n",
    "        return self.M_list[idx].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced36a61-e293-440f-9548-8c165b771958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 121.771523\n",
      "Epoch [100/1000], Loss: 20.060699\n",
      "Epoch [200/1000], Loss: 28.634759\n",
      "Epoch [300/1000], Loss: 9.888518\n",
      "Epoch [400/1000], Loss: 6.115332\n",
      "Epoch [500/1000], Loss: 9.561793\n",
      "Epoch [600/1000], Loss: 3.156965\n",
      "Epoch [700/1000], Loss: 2.499913\n",
      "Epoch [800/1000], Loss: 3.814348\n",
      "Epoch [900/1000], Loss: 2.437309\n",
      "Epoch [1000/1000], Loss: 3.260542\n",
      "\n",
      "--- Solution 1 ---\n",
      "Generated N array:\n",
      "tensor([ 3.9949, -2.1573, -2.8973])\n",
      "\n",
      "Learned parameters G:\n",
      "tensor([[ 0.4030,  2.1078, -0.4830],\n",
      "        [ 0.7256, -2.4330, -1.4904],\n",
      "        [ 0.0470, -0.1597,  0.5669]])\n",
      "\n",
      "--- Solution 2 ---\n",
      "Generated N array:\n",
      "tensor([ 4.4981, -2.4991, -3.0916])\n",
      "\n",
      "Learned parameters G:\n",
      "tensor([[ 0.4030,  2.1078, -0.4830],\n",
      "        [ 0.7256, -2.4330, -1.4904],\n",
      "        [ 0.0470, -0.1597,  0.5669]])\n",
      "\n",
      "--- Solution 3 ---\n",
      "Generated N array:\n",
      "tensor([ 7.1132, -4.6102, -3.6999])\n",
      "\n",
      "Learned parameters G:\n",
      "tensor([[ 0.4030,  2.1078, -0.4830],\n",
      "        [ 0.7256, -2.4330, -1.4904],\n",
      "        [ 0.0470, -0.1597,  0.5669]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Computes the VAE loss function as a combination of reconstruction loss and KL divergence.\n",
    "\n",
    "    Args:\n",
    "        recon_x (torch.Tensor): Reconstructed M means.\n",
    "        x (torch.Tensor): Original M means.\n",
    "        mu (torch.Tensor): Mean from the encoder.\n",
    "        logvar (torch.Tensor): Log variance from the encoder.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Combined loss.\n",
    "    \"\"\"\n",
    "    mse_loss = nn.MSELoss()(recon_x, x)\n",
    "    # KL Divergence\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return mse_loss + KLD\n",
    "\n",
    "def train_vae_model(M_list, n, m, latent_dim=10, batch_size=32, epochs=1000, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Trains the VAE model to learn the distribution of N arrays and G parameters.\n",
    "\n",
    "    Args:\n",
    "        M_list (list of torch.Tensor): List of M matrices, each of shape (n, m).\n",
    "        n (int): Number of rows in each M matrix.\n",
    "        m (int): Number of columns in each M matrix (same as length of N arrays).\n",
    "        latent_dim (int, optional): Dimension of the latent space. Defaults to 10.\n",
    "        batch_size (int, optional): Batch size for training. Defaults to 32.\n",
    "        epochs (int, optional): Number of training epochs. Defaults to 1000.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-3.\n",
    "\n",
    "    Returns:\n",
    "        VAE: Trained VAE model.\n",
    "    \"\"\"\n",
    "    dataset = PolynomialVaeDataset(M_list)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = VAE(m, latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar, N = model(batch)\n",
    "            loss = loss_function(recon, batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == 1:\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.6f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_multiple_solutions(model, num_samples=5):\n",
    "    \"\"\"\n",
    "    Generates multiple N and G solutions from the trained VAE model.\n",
    "\n",
    "    Args:\n",
    "        model (VAE): Trained VAE model.\n",
    "        num_samples (int, optional): Number of solutions to generate. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains an N tensor and the corresponding G tensor.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    solutions = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            z = torch.randn(1, model.latent_dim).to(device)\n",
    "            N = model.decode(z)  # Shape: (1, m)\n",
    "            G = model.G  # Shape: (3, m)\n",
    "            solutions.append((N.squeeze(0).cpu(), G.detach().cpu()))\n",
    "    return solutions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "\n",
    "    # Define dimensions\n",
    "    n, m = 4, 3  # For example, 4x3 matrices and 3-dimensional N arrays\n",
    "\n",
    "    # Create example M matrices (without knowing N)\n",
    "    M_list = [\n",
    "        torch.tensor([\n",
    "            [1.0, 2.0, 3.0],\n",
    "            [2.0, 4.0, 6.0],\n",
    "            [3.0, 6.0, 9.0],\n",
    "            [4.0, 8.0, 12.0]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [2.0, 3.0, 4.0],\n",
    "            [4.0, 6.0, 8.0],\n",
    "            [6.0, 9.0, 12.0],\n",
    "            [8.0, 12.0, 16.0]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [3.0, 4.0, 5.0],\n",
    "            [6.0, 8.0, 10.0],\n",
    "            [9.0, 12.0, 15.0],\n",
    "            [12.0, 16.0, 20.0]\n",
    "        ])\n",
    "    ]\n",
    "\n",
    "    # Train the VAE model\n",
    "    trained_vae = train_vae_model(M_list, n, m, latent_dim=10, batch_size=2, epochs=1000, lr=1e-3)\n",
    "\n",
    "    # Generate multiple solutions\n",
    "    num_solutions = 3  # Specify how many solutions you want\n",
    "    solutions = generate_multiple_solutions(trained_vae, num_samples=num_solutions)\n",
    "\n",
    "    for idx, (N, G) in enumerate(solutions):\n",
    "        print(f\"\\n--- Solution {idx+1} ---\")\n",
    "        print(\"Generated N array:\")\n",
    "        print(N)\n",
    "        print(\"\\nLearned parameters G:\")\n",
    "        print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5393cd2-f8eb-451d-b05f-15bc52cc6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_multiple_solutions(trained_vae, num_samples=num_solutions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
